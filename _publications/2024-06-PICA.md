---
title: "Take Off the Training Wheels! Progressive In-Context Learning for Effective Alignment"
collection: publications
permalink: /publication/2024-06-PICA
excerpt: '本工作通过上下文向量和递进式生成策略实现了高效的无需训练的模型对齐方法。通过比较Few-shot和Zero-shot设置下模型对对齐数据的表示的差异，我们发现模型响应的前几个token会极大地影响整个响应的质量。受此启发，我们提出了递进式生成策略，在对齐时候在Few-shot设置下生成前几个token，之后在Zero-shot设置下补全响应，极大的提高响应的生成速度。我们还通过上下文向量增强Zero-shot设置下模型的生成的质量，并且几乎不影响生成速度。基于我们之前的上下文向量的研究，我们成功通过上下文学习的方式使模型对齐人类意图，无需指令微调即可激发模型的指令遵循能力。'
venue: 'Arxiv'
date: 2024-06-16
---

**Abstract**

Recent studies have explored the working mechanisms of In-Context Learning (ICL). However, they mainly focus on classification and simple generation tasks, limiting their broader application to more complex generation tasks in practice. To address this gap, we investigate the impact of demonstrations on token representations within the practical alignment tasks. We find that the transformer embeds the task function learned from demonstrations into the separator token representation, which plays an important role in the generation of prior response tokens. Once the prior response tokens are determined, the demonstrations become redundant. Motivated by this finding, we propose an efficient Progressive In-Context Alignment (PICA) method consisting of two stages. In the first few-shot stage, the model generates several prior response tokens via standard ICL while concurrently extracting the ICL vector that stores the task function from the separator token representation. In the following zero-shot stage, this ICL vector guides the model to generate responses without further demonstrations. Extensive experiments demonstrate that our  \textsc{PICA} not only surpasses vanilla ICL but also achieves comparable performance to other alignment tuning methods. The proposed training-free method reduces the time cost (e.g., 5.45×) with improved alignment performance (e.g., 6.57+). Consequently, our work highlights the application of ICL for alignment and calls for a deeper understanding of ICL for complex generations.
